# humanoid_planner_deepseek_async.py

import os
import json
import asyncio
import aiohttp
from typing import Dict, List, Optional, Any, AsyncGenerator
from humanoid_prompt_template import (
    get_humanoid_system_prompt,
    get_humanoid_test_cases,
    validate_response,
    clean_json_response
)
from deepseek_config import get_deepseek_config, list_available_presets

class HumanoidRobotPlannerDeepSeekAsync:
    """
    Unitree-G1 Humanoid Robot Planner - DeepSeek API Async Version
    æ”¯æŒå¼‚æ­¥è°ƒç”¨å’Œæµå¼è¾“å‡º
    """

    def __init__(self, preset: str = "balanced", endpoint: str = "primary"):
        """
        åˆå§‹åŒ–äººå½¢æœºå™¨äººè§„åˆ’å™¨ï¼ˆDeepSeekå¼‚æ­¥ç‰ˆæœ¬ï¼‰

        Args:
            preset: å‚æ•°é¢„è®¾ ('precise', 'balanced', 'creative', 'fast')
            endpoint: APIç«¯ç‚¹é€‰æ‹© ('primary', 'backup')
        """
        # è·å–é…ç½®
        self.config = get_deepseek_config(preset, endpoint)
        self.base_url = self.config["base_url"]
        self.model = self.config["model"]
        self.preset = preset
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            "Accept": "application/json",
            "Content-Type": "application/json"
        }
        
        # åˆå§‹åŒ–sessionï¼ˆå°†åœ¨async contextä¸­åˆ›å»ºï¼‰
        self.session = None
        
        self.system_prompt = get_humanoid_system_prompt()
        print(f"âœ… Initialized Async HumanoidRobotPlanner")
        print(f"   Model: {self.model}")
        print(f"   Preset: {preset}")
        print(f"   Endpoint: {self.base_url}")

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()

    async def _make_api_request(self, messages: List[Dict], stream: bool = False) -> Dict:
        """
        å¼‚æ­¥å‘é€APIè¯·æ±‚åˆ°DeepSeekæœåŠ¡

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º

        Returns:
            APIå“åº”
        """
        if not self.session:
            self.session = aiohttp.ClientSession()
        
        request_data = {
            "model": self.model,
            "messages": messages,
            "max_tokens": self.config["max_tokens"],
            "temperature": self.config["temperature"],
            "top_p": self.config["top_p"],
            "presence_penalty": self.config["presence_penalty"],
            "frequency_penalty": self.config["frequency_penalty"],
            "stream": stream
        }
        
        try:
            timeout = aiohttp.ClientTimeout(total=self.config.get("timeout", 60))
            
            async with self.session.post(
                self.base_url,
                headers=self.headers,
                json=request_data,
                timeout=timeout
            ) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"API returned status {response.status}: {error_text}")
                
                if stream:
                    return response  # è¿”å›å“åº”å¯¹è±¡ç”¨äºæµå¼å¤„ç†
                else:
                    return await response.json()
                    
        except asyncio.TimeoutError:
            raise Exception("API request timeout")
        except Exception as e:
            raise Exception(f"API request failed: {str(e)}")

    async def _handle_stream_response(self, messages: List[Dict]) -> str:
        """
        å¤„ç†æµå¼å“åº”
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            å®Œæ•´çš„å“åº”æ–‡æœ¬
        """
        print("Streaming response:")
        full_response = ""
        
        try:
            response = await self._make_api_request(messages, stream=True)
            
            async for line in response.content:
                line = line.decode('utf-8').strip()
                if line.startswith('data: '):
                    data_str = line[6:]
                    if data_str == '[DONE]':
                        break
                    
                    try:
                        data = json.loads(data_str)
                        if 'choices' in data and len(data['choices']) > 0:
                            delta = data['choices'][0].get('delta', {})
                            content = delta.get('content', '')
                            if content:
                                print(content, end='', flush=True)
                                full_response += content
                    except json.JSONDecodeError:
                        continue
            
            print("\n")
            return full_response
            
        except Exception as e:
            print(f"\nâŒ Stream processing error: {str(e)}")
            return full_response if full_response else ""

    async def plan_task(self, human_request: str, return_raw: bool = False, 
                       stream: bool = False, debug: bool = False) -> Dict:
        """
        å¼‚æ­¥ç”Ÿæˆäººå½¢æœºå™¨äººä»»åŠ¡æ‰§è¡Œè®¡åˆ’

        Args:
            human_request: äººç±»çš„è‡ªç„¶è¯­è¨€è¯·æ±‚
            return_raw: æ˜¯å¦è¿”å›åŸå§‹å“åº”æ–‡æœ¬
            stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
            debug: æ˜¯å¦æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯

        Returns:
            dict: è§£æåçš„ä»»åŠ¡è®¡åˆ’ï¼Œæˆ–åŸå§‹å“åº”æ–‡æœ¬
        """
        try:
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": human_request}
            ]
            
            if debug:
                print(f"Sending async request to DeepSeek API...")
                print(f"Model: {self.model}")
                print(f"Preset: {self.preset}")
                print(f"Temperature: {self.config['temperature']}")
            
            # è°ƒç”¨DeepSeek API
            if stream:
                response_text = await self._handle_stream_response(messages)
            else:
                response = await self._make_api_request(messages, stream=False)
                
                # æå–å“åº”å†…å®¹
                if 'choices' in response and len(response['choices']) > 0:
                    response_text = response['choices'][0]['message']['content']
                else:
                    raise Exception("Invalid API response format")
            
            if debug:
                print(f"Raw API response:\n{response_text}\n{'='*50}")
            
            if return_raw:
                return response_text
            
            # éªŒè¯å’Œè§£æå“åº”
            return self._parse_response(response_text)
            
        except Exception as e:
            return {"error": f"API call failed: {str(e)}"}

    def _parse_response(self, response_text: str) -> Dict:
        """è§£æå“åº”æ–‡æœ¬"""
        cleaned_text = clean_json_response(response_text)
        
        is_valid, message = validate_response(response_text)
        if not is_valid:
            print(f"Warning: {message}")
            try:
                return json.loads(cleaned_text)
            except json.JSONDecodeError:
                return {"error": message, "raw_response": response_text}
        
        try:
            return json.loads(cleaned_text)
        except json.JSONDecodeError as e:
            print(f"JSON parsing failed: {str(e)}")
            return {"error": f"JSON parsing failed: {str(e)}", "raw_response": response_text}

    async def batch_plan_tasks(self, requests: List[str], max_concurrent: int = 3) -> List[Dict]:
        """
        æ‰¹é‡å¼‚æ­¥å¤„ç†å¤šä¸ªä»»åŠ¡è¯·æ±‚
        
        Args:
            requests: ä»»åŠ¡è¯·æ±‚åˆ—è¡¨
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            ä»»åŠ¡è®¡åˆ’åˆ—è¡¨
        """
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def process_with_limit(request: str, index: int) -> Dict:
            async with semaphore:
                print(f"Processing request {index + 1}/{len(requests)}: {request[:50]}...")
                result = await self.plan_task(request)
                return {"index": index, "request": request, "result": result}
        
        tasks = [process_with_limit(req, i) for i, req in enumerate(requests)]
        results = await asyncio.gather(*tasks)
        
        # æŒ‰åŸå§‹é¡ºåºæ’åº
        results.sort(key=lambda x: x['index'])
        return [r['result'] for r in results]

    async def run_tests_async(self, stream: bool = False):
        """å¼‚æ­¥è¿è¡Œæµ‹è¯•ç”¨ä¾‹"""
        test_cases = get_humanoid_test_cases()
        print(f"\nğŸ§ª Running {len(test_cases)} test cases asynchronously")
        print(f"Model: {self.model}")
        print(f"Preset: {self.preset}")
        print("="*60)

        results = {"passed": 0, "failed": 0, "errors": 0}
        
        # å‡†å¤‡æ‰€æœ‰æµ‹è¯•è¯·æ±‚
        test_requests = [test['request'] for test in test_cases]
        
        # æ‰¹é‡å¤„ç†
        if not stream:
            print("Running tests in batch mode...")
            plans = await self.batch_plan_tasks(test_requests, max_concurrent=3)
            
            for i, (test_case, plan) in enumerate(zip(test_cases, plans), 1):
                print(f"\n[Test {i}/{len(test_cases)}] {test_case['id']}")
                
                if "error" not in plan:
                    complexity = plan['task_analysis']['complexity']
                    expected = test_case['expected_complexity']
                    
                    if complexity == expected:
                        print(f"âœ… PASSED - Complexity: {complexity}")
                        results["passed"] += 1
                    else:
                        print(f"âš ï¸  MISMATCH - Got: {complexity}, Expected: {expected}")
                        results["failed"] += 1
                else:
                    print(f"âŒ ERROR: {plan['error']}")
                    results["errors"] += 1
        else:
            # æµå¼æ¨¡å¼ä¸‹é€ä¸ªå¤„ç†
            for i, test_case in enumerate(test_cases, 1):
                print(f"\n[Test {i}/{len(test_cases)}] {test_case['id']}")
                print(f"Request: {test_case['request']}")
                
                plan = await self.plan_task(test_case['request'], stream=True)
                
                if "error" not in plan:
                    complexity = plan['task_analysis']['complexity']
                    expected = test_case['expected_complexity']
                    
                    if complexity == expected:
                        print(f"âœ… PASSED - Complexity: {complexity}")
                        results["passed"] += 1
                    else:
                        print(f"âš ï¸  MISMATCH - Got: {complexity}, Expected: {expected}")
                        results["failed"] += 1
                else:
                    print(f"âŒ ERROR: {plan['error']}")
                    results["errors"] += 1

        # æµ‹è¯•ç»“æœæ€»ç»“
        print(f"\nğŸ“Š TEST SUMMARY:")
        print(f"âœ… Passed: {results['passed']}")
        print(f"âš ï¸  Failed: {results['failed']}")
        print(f"âŒ Errors: {results['errors']}")
        total = sum(results.values())
        success_rate = (results['passed'] / total * 100) if total > 0 else 0
        print(f"ğŸ¯ Success Rate: {success_rate:.1f}%")

    def execute_plan(self, plan: Dict):
        """æ‰§è¡Œè®¡åˆ’ï¼ˆä¸åŒæ­¥ç‰ˆæœ¬ç›¸åŒï¼‰"""
        if "error" in plan:
            print(f"âŒ Cannot execute plan due to error: {plan['error']}")
            if "raw_response" in plan:
                print(f"Raw response: {plan['raw_response'][:200]}...")
            return

        print("\n" + "="*60)
        print("ğŸ¤– HUMANOID ROBOT (Unitree-G1) - TASK EXECUTION")
        print("="*60)

        task_analysis = plan['task_analysis']
        print(f"ğŸ“‹ Task Intent: {task_analysis['intent']}")
        print(f"ğŸ·ï¸  Entities: {', '.join(task_analysis['entities'])}")
        print(f"âš¡ Complexity: {task_analysis['complexity']}")
        print(f"â±ï¸  Estimated Duration: {task_analysis['estimated_duration']}")

        print(f"\nğŸ“ EXECUTION PLAN ({len(plan['execution_plan'])} steps):")
        for step in plan['execution_plan']:
            print(f"\n  Step {step['step']}: ğŸ¤– {step['agent']}")
            print(f"    ğŸ“ Location: {step['location']}")
            print(f"    ğŸ¯ Action: {step['action']}")
            if step.get('parameters'):
                print(f"    âš™ï¸  Parameters: {step['parameters']}")
            if step.get('dependencies'):
                print(f"    ğŸ”— Dependencies: Step {', '.join(map(str, step['dependencies']))}")
            print(f"    âœ… Success Criteria: {step['success_criteria']}")

        if plan.get('contingency_plans'):
            print(f"\nğŸš¨ CONTINGENCY PLANS:")
            for i, contingency in enumerate(plan['contingency_plans'], 1):
                print(f"  {i}. If {contingency['failure_scenario']}")
                print(f"     â†’ {contingency['alternative_action']}")

        print(f"\nğŸ’¬ HUMANOID ROBOT FEEDBACK: {plan['human_feedback']}")
        print("="*60)

async def main():
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    print("ğŸ¤– Humanoid Robot Planner - DeepSeek Async Edition")
    print("="*60)

    # æ˜¾ç¤ºå¯ç”¨é¢„è®¾
    list_available_presets()
    
    # è®©ç”¨æˆ·é€‰æ‹©é¢„è®¾
    print("\nSelect a preset (or press Enter for 'balanced'):")
    preset_choice = input("Preset: ").strip().lower() or "balanced"
    
    async with HumanoidRobotPlannerDeepSeekAsync(preset=preset_choice) as planner:
        print(f"\nğŸ“‹ Available Commands:")
        print("  - Type your request in natural language")
        print("  - 'test': Run test cases")
        print("  - 'test stream': Run tests with streaming")
        print("  - 'batch': Test batch processing")
        print("  - 'debug <request>': Run with debug output")
        print("  - 'presets': Show available presets")
        print("  - 'quit' or 'q': Exit")

        while True:
            user_input = input(f"\nğŸ¤– DeepSeek [{preset_choice}] > ").strip()

            if user_input.lower() in ['quit', 'q']:
                print("ğŸ‘‹ Goodbye!")
                break
            elif user_input.lower() == 'test':
                await planner.run_tests_async()
            elif user_input.lower() == 'test stream':
                await planner.run_tests_async(stream=True)
            elif user_input.lower() == 'batch':
                test_requests = [
                    "å¸®æˆ‘æ‹¿æ¯æ°´",
                    "æ‰“æ‰«å®¢å…",
                    "æ‰¾åˆ°æˆ‘çš„æ‰‹æœº"
                ]
                print(f"Testing batch processing with {len(test_requests)} requests...")
                results = await planner.batch_plan_tasks(test_requests)
                for i, result in enumerate(results, 1):
                    print(f"\nResult {i}:")
                    if "error" not in result:
                        print(f"  âœ… Success: {result['task_analysis']['intent']}")
                    else:
                        print(f"  âŒ Error: {result['error']}")
            elif user_input.lower() == 'presets':
                list_available_presets()
            elif user_input.lower().startswith('debug '):
                request = user_input[6:].strip()
                plan = await planner.plan_task(request, debug=True)
                planner.execute_plan(plan)
            elif user_input:
                use_stream = input("Use streaming? (y/N): ").lower().startswith('y')
                plan = await planner.plan_task(user_input, stream=use_stream)
                planner.execute_plan(plan)

if __name__ == "__main__":
    asyncio.run(main())